{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorCores': 4, 'executorMemory': '47969M', 'conf': {'spark.task.cpus': 4}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1565254661534_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-3-197.eu-west-2.compute.internal:20888/proxy/application_1565254661534_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-99.eu-west-2.compute.internal:8042/node/containerlogs/container_1565254661534_0002_01_000001/livy\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"executorCores\": 4, \"executorMemory\": \"47969M\", \"conf\":{\"spark.task.cpus\": 4}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1565254661534_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-3-197.eu-west-2.compute.internal:20888/proxy/application_1565254661534_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-61.eu-west-2.compute.internal:8042/node/containerlogs/container_1565254661534_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import html2text\n",
    "from smart_open import smart_open\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "import json\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "import html2text\n",
    "from urllib.parse import urlparse, ParseResult\n",
    "from scrapy.linkextractors import IGNORED_EXTENSIONS\n",
    "from scrapy.selector import Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_file = \"s3://onai-ml-dev-eu-west-1/web_crawler/data/seed_urls/company-urls-by-country.csv\"\n",
    "\n",
    "def extract_text_from_html(html, upper_bound=10000):\n",
    "    if len(html.split(\"\\n\")) > upper_bound:\n",
    "        return \"\"\n",
    "    parser = html2text.HTML2Text()\n",
    "    parser.wrap_links = False\n",
    "    parser.skip_internal_links = True\n",
    "    parser.inline_links = True\n",
    "    parser.ignore_anchors = True\n",
    "    parser.ignore_images = True\n",
    "    parser.ignore_emphasis = True\n",
    "    parser.ignore_links = True\n",
    "    return parser.handle(html)\n",
    "\n",
    "def convert_url(url):\n",
    "    p = urlparse(url, 'http')\n",
    "    netloc = p.netloc or p.path\n",
    "    path = p.path if p.netloc else ''\n",
    "    if not netloc.startswith('www.'):\n",
    "        netloc = 'www.' + netloc\n",
    "\n",
    "    p = ParseResult('http', netloc, path, *p[3:])\n",
    "    return p.geturl()\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"my_spider\"\n",
    "\n",
    "    # crawl in BFS\n",
    "    custom_settings = {\n",
    "        'DEPTH_LIMIT': 2,\n",
    "        'DEPTH_PRIORITY': 1,\n",
    "        'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue',\n",
    "        'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue',\n",
    "        'SCHEDULER_PRIORITY_QUEUE': 'scrapy.pqueues.DownloaderAwarePriorityQueue',\n",
    "        'DNS_TIMEOUT': 20,\n",
    "        'DOWNLOAD_TIMEOUT': 30,\n",
    "        'LOG_LEVEL': 'ERROR',\n",
    "        'CONCURRENT_REQUESTS': 100,\n",
    "        'REACTOR_THREADPOOL_MAXSIZE': 20,        \n",
    "        'REDIRECT_ENABLED': False,\n",
    "        'RETRY_ENABLED': False,\n",
    "    }\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        start_urls_and_ids = kwargs.get('start_urls_and_ids')\n",
    "        start_urls = [start_url for _,start_url in start_urls_and_ids]\n",
    "\n",
    "        parsed_urls = [urlparse(start_url) for start_url in start_urls]\n",
    "        url_domains = [parsed_url.netloc for parsed_url in parsed_urls]\n",
    "        \n",
    "        self.company_ids = {start_url:company_id for company_id,start_url in start_urls_and_ids}\n",
    "        self.allowed_domains = url_domains\n",
    "        \n",
    "        print(self.company_ids)\n",
    "\n",
    "        self.output = kwargs.get('output')\n",
    "\n",
    "        self.allowed_extensions = ('.pdf', '.doc', '.xls', '.docx', '.xlsx')\n",
    "        self.denied_extensions = tuple(set(['.' + ext for ext in IGNORED_EXTENSIONS]) - set(self.allowed_extensions))\n",
    "\n",
    "        super(MySpider, self).__init__(*args, **kwargs, start_urls=start_urls)\n",
    "\n",
    "    def save_file(self, response, upper_limit=2000000): # ~ 2 mbs tops\n",
    "        page_url = response.meta.get('page_url')\n",
    "        root_page = response.meta.get('root_page')\n",
    "        company_id = self.company_ids[root_page]\n",
    "        \n",
    "        path = response.url.split('/')[-1]\n",
    "        filename = path[path.rfind(\"/\") + 1:]\n",
    "        if len(response.body) < upper_limit:\n",
    "            self.output.append((company_id, \n",
    "                                root_page, \n",
    "                                description, \n",
    "                                page_url, \n",
    "                                filename, \n",
    "                                \"file\", \n",
    "                                bytearray(response.body)))\n",
    "\n",
    "    def parse(self, response):\n",
    "        title_xpath = response.selector.xpath('//title/text()').extract()\n",
    "        page_title = title_xpath[0] if title_xpath else \"\"\n",
    "        \n",
    "        root_page = response.meta.get(\"root_page\")\n",
    "        if root_page is None:\n",
    "            root_page = response.url\n",
    "            \n",
    "        company_id = self.company_ids[root_page]\n",
    "\n",
    "        description = response.meta.get(\"root_description\")\n",
    "        if description is None:\n",
    "            description_xpath = response.selector.xpath('//meta[@name=\\'description\\']/@content').extract()\n",
    "            description = description_xpath[0] if description_xpath else \"\"\n",
    "\n",
    "        page_content = extract_text_from_html(response.text)\n",
    "        self.log('Page: %s (%s)' % (response.url, page_title))\n",
    "\n",
    "        extracted_links = LinkExtractor().extract_links(response)\n",
    "        for link in extracted_links:\n",
    "            url = link.url\n",
    "            parsed_url = urlparse(url)\n",
    "            url_domain = parsed_url.netloc\n",
    "            \n",
    "            follow_link = False\n",
    "            for marker in ['who we are', 'overview', 'about', 'mission']:\n",
    "                if marker in link.url.lower():\n",
    "                    follow_link = True\n",
    "\n",
    "            for marker in ['who we are', 'overview', 'about', 'mission']:\n",
    "                if marker in link.text.lower():\n",
    "                    follow_link = True\n",
    "                    \n",
    "            if not follow_link:\n",
    "                continue\n",
    "\n",
    "            if url_domain in self.allowed_domains:\n",
    "                if link.url.endswith(self.allowed_extensions):\n",
    "                    yield scrapy.Request(url,\n",
    "                                         callback=self.save_file,\n",
    "                                         meta={'page_url': response.url, \n",
    "                                               'root_page':root_page,\n",
    "                                               'root_description': description\n",
    "                                              }\n",
    "                                        )\n",
    "                elif link.url.endswith(self.denied_extensions):\n",
    "                    continue\n",
    "                else:\n",
    "                    yield scrapy.Request(url,\n",
    "                                         callback=self.parse,\n",
    "                                         meta={'root_page': root_page,\n",
    "                                               'root_description': description\n",
    "                                              }\n",
    "                                        )\n",
    "        \n",
    "        self.output.append((company_id,\n",
    "                            root_page,\n",
    "                            description,\n",
    "                            response.url,\n",
    "                            page_title,\n",
    "                            \"html\",\n",
    "                            page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7309166"
     ]
    }
   ],
   "source": [
    "spark.read.csv(input_file, header=True, inferSchema=True, sep=\"\\t\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = (spark.read.csv(input_file, header=True, inferSchema=True, sep=\"\\t\")\n",
    "      .withColumnRenamed(\"web_page\", \"url\")\n",
    "      .filter(F.col(\"url\").isNotNull())\n",
    "      .repartition(1000)\n",
    "      .distinct()\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1430447"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.parquet(\"s3://onai-ml-dev-eu-west-1/web_crawler/data/seed_urls/company_urls.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convert_url_udf = F.udf(convert_url, T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.load(\"s3://onai-ml-dev-eu-west-1/web_crawler/data/seed_urls/company_urls.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- company_id: integer (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- country: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1430447"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_spider(urls):\n",
    "    from twisted.internet import reactor\n",
    "    from scrapy.crawler import CrawlerRunner\n",
    "    \n",
    "    from crochet import setup, wait_for, TimeoutError\n",
    "    setup()\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    @wait_for(timeout=200.0)\n",
    "    def run_it_already():\n",
    "        crawler = CrawlerRunner()\n",
    "        return crawler.crawl(MySpider, \n",
    "                             start_urls_and_ids=[(company_id, convert_url(url)) for company_id,url in urls], \n",
    "                             output=data\n",
    "                            )\n",
    "    try:\n",
    "        run_it_already()\n",
    "    except TimeoutError:\n",
    "        # In case of timeout, just return what we managed to scrape\n",
    "        print(\"Timed out for given URLs\", list(urls))\n",
    "        print(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_csv(\"s3://onai-ml-dev-eu-west-1/web_crawler/data/seed_urls/company-urls.csv\", header=0, sep='\\t').astype(str)\n",
    "tst_urls = pandas_df[\"url\"].iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data1 = run_spider(tst_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in data1:\n",
    "    if \"http://www.thetacapital.com\" in el[2]:\n",
    "        print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crawl_df = spark.createDataFrame(df\n",
    "            .repartition(1000)\n",
    "            .rdd\n",
    "            .mapPartitions(lambda u: run_spider((el.company_id, el.url) for el in u))\n",
    "            , T.StructType([\n",
    "                T.StructField(\"company_id\", T.StringType()),\n",
    "                T.StructField(\"website\", T.StringType()),\n",
    "                T.StructField(\"description\", T.StringType()),\n",
    "                T.StructField(\"url\", T.StringType()),\n",
    "                T.StructField(\"page_title\", T.StringType()),\n",
    "                T.StructField(\"content_type\", T.StringType()),\n",
    "                T.StructField(\"raw_content\", T.StringType())\n",
    "            ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- company_id: string (nullable = true)\n",
      " |-- website: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- page_title: string (nullable = true)\n",
      " |-- content_type: string (nullable = true)\n",
      " |-- raw_content: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "crawl_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(crawl_df\n",
    " .repartition(100)\n",
    " .write\n",
    " .parquet(\"s3://onai-ml-dev-eu-west-1/web_crawler/data/raw_crawl_results\", mode=\"overwrite\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crawl_results = spark.read.load(\"s3://onai-ml-dev-eu-west-1/web_crawler/data/raw_crawl_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014614"
     ]
    }
   ],
   "source": [
    "crawl_results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- company_id: string (nullable = true)\n",
      " |-- website: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- page_title: string (nullable = true)\n",
      " |-- content_type: string (nullable = true)\n",
      " |-- raw_content: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "crawl_results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- company_id: integer (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- country: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_companies = (crawl_results.join(df,\n",
    "                                       on = [\"company_id\"]\n",
    "                                      )\n",
    "                    .drop(df.url)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014614"
     ]
    }
   ],
   "source": [
    "joined_companies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_companies.write.parquet(\"s3://onai-ml-dev-eu-west-1/web_crawler/data/urls_and_content\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst = spark.read.load(\"s3://onai-ml-dev-eu-west-1/web_crawler/data/urls_and_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014614"
     ]
    }
   ],
   "source": [
    "tst.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- company_id: string (nullable = true)\n",
      " |-- website: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- page_title: string (nullable = true)\n",
      " |-- content_type: string (nullable = true)\n",
      " |-- raw_content: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- country: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "tst.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tst\n",
    " .repartition(100)\n",
    " .write\n",
    " .parquet(\"s3://onai-ml-dev-eu-west-1/web_crawler/data/sample_urls_and_content\", mode=\"overwrite\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "* 10 urls per partition ~ 2 mins per partition: takes about 1.5 hrs to finish 10k urls\n",
    "* 100 urls per partition and config fixes: 21 minutes\n",
    "* 1000 urls per partition: 17 mins\n",
    "* looking only at main page and about, takes 8 mins 100 docs per partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
