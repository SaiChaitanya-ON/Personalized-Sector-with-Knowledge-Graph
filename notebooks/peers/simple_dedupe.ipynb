{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"executorCores\": 4, \"executorMemory\": \"8096M\", \"conf\": {\"spark.default.parallelism\": 1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_aws_credentials():\n",
    "#     \"\"\"\n",
    "#         Loads AWS credentials from credentials file and sets env variables\n",
    "#     \"\"\"\n",
    "#     credentials_file = os.getenv(\"HOME\") + '/.aws/credentials'\n",
    "\n",
    "#     try:\n",
    "#         with open(credentials_file, 'r') as f:\n",
    "#             lines = f.read()\n",
    "#         config = dict([tuple([kv.strip() for kv in line.split('=')]) for line in lines.split('\\n') if '=' in line])\n",
    "#     except FileNotFoundError:\n",
    "#         config = dict()\n",
    "#     if \"aws_access_key_id\" in config and \"aws_secret_access_key\" in config:\n",
    "#         return config[\"aws_access_key_id\"], config[\"aws_secret_access_key\"]\n",
    "#     elif \"AWS_ACCESS_KEY_ID\" in os.environ and \"AWS_SECRET_ACCESS_KEY\" in os.environ:\n",
    "#         return os.environ[\"AWS_ACCESS_KEY_ID\"], os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "#     return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Set Spark Home\n",
    "# os.environ[\"SPARK_HOME\"] = \"/Users/weikun.wang/dev/spark-2.4.3-bin-hadoop2.7\"\n",
    "\n",
    "# # Get AWS credentials\n",
    "# aws_access_key_id, aws_secret_access_key = get_aws_credentials()\n",
    "\n",
    "# # Create Spark Session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .appName(\"SparkTest\") \\\n",
    "#     .config(\"spark.driver.host\", '127.0.0.1') \\\n",
    "#     .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "#     .config(\"spark.hadoop.fs.AbstractFileSystem.s3.impl\", \"org.apache.hadoop.fs.s3a.S3A\") \\\n",
    "#     .config(\"spark.hadoop.fs.AbstractFileSystem.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3A\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3.endpoint\", \"s3.amazonaws.com\") \\\n",
    "#     .config(\"spark.executor.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\") \\\n",
    "#     .config(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\") \\\n",
    "#     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "#     .config(\"spark.driver.memory\", \"6g\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3.access.key\", aws_access_key_id) \\\n",
    "#     .config(\"spark.hadoop.fs.s3.secret.key\", aws_secret_access_key) \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key_id) \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_access_key) \\\n",
    "#     .getOrCreate()\n",
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# import math\n",
    "# import gensim\n",
    "# import random\n",
    "# import numpy as np\n",
    "# from gensim.utils import tokenize\n",
    "# import string\n",
    "# from unidecode import unidecode\n",
    "# from langdetect import detect\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType, IntegerType, ArrayType, DoubleType, MapType, BooleanType\n",
    "# from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, CountVectorizer\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# import pyspark.sql.functions as F\n",
    "# import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from unidecode import unidecode\n",
    "from gensim.utils import tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType, DoubleType, MapType\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"s3://ai-data-lake-dev-eu-west-1/business/company_data_denormalized\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "normalize_text_udf = udf(normalize_text, StringType())\n",
    "\n",
    "df = df.withColumn(\"name_normalized\", normalize_text_udf(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = [item[0] for item in df.dtypes if item[1].startswith('string') and 'id' not in item[0]]\n",
    "double_columns = [item[0] for item in df.dtypes if item[1].startswith('double')]\n",
    "integer_columns = [item[0] for item in df.dtypes if item[1].startswith('int') and 'id' not in item[0]]\n",
    "list_columns = \"id_capiq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupByKey = [\"name_normalized\", \"country_of_incorporation\", \"region\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = [F.collect_list(colName).alias(colName) for colName in df.columns if colName not in groupByKey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupBy(groupByKey).agg(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_grouped_small = df_grouped.limit(1000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [type(np.mean(x)) for x in pd_df['total_revenue'] if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df_grouped_small.select('entity_id').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_string_fields(field):\n",
    "    if isinstance(field, list):\n",
    "        return \", \".join(field)\n",
    "    else:\n",
    "        return field\n",
    "    \n",
    "def transform_double_fields(field):\n",
    "    if isinstance(field, list):\n",
    "        return float(np.mean(field)) if len(field) > 0 else [0.0]\n",
    "    else:\n",
    "        return field\n",
    "    \n",
    "def transform_integer_fields(field):\n",
    "    if isinstance(field, list):\n",
    "        return int(np.mean(field)) if len(field) > 0 else 0.0\n",
    "    else:\n",
    "        return field\n",
    "    \n",
    "def transform_entity_id(field):\n",
    "    if isinstance(field, list):\n",
    "        return field[0]\n",
    "    else:\n",
    "        return field\n",
    "\n",
    "transform_string_fields_udf = udf(transform_string_fields, StringType())\n",
    "transform_double_fields_udf = udf(transform_double_fields, DoubleType())\n",
    "transform_integer_fields_udf = udf(transform_integer_fields, IntegerType())\n",
    "transform_entity_id_udf = udf(transform_entity_id, StringType())\n",
    "\n",
    "for col_name in string_columns:\n",
    "    df_grouped = df_grouped.withColumn(col_name, transform_string_fields_udf(col_name))\n",
    "    \n",
    "for col_name in double_columns:\n",
    "    df_grouped = df_grouped.withColumn(col_name, transform_double_fields_udf(col_name))\n",
    "    \n",
    "for col_name in integer_columns:\n",
    "    df_grouped = df_grouped.withColumn(col_name, transform_integer_fields_udf(col_name))\n",
    "    \n",
    "df_grouped = df_grouped.withColumn(\"entity_id\", transform_entity_id_udf(F.col(\"entity_id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_grouped\n",
    " .write\n",
    " .parquet(\"s3://ai-data-lake-dev-eu-west-1/staging/peer/company_data_denormalized_deduped\", mode=\"overwrite\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = spark.read.load(\"s3://ai-data-lake-dev-eu-west-1/staging/peer/company_data_denormalized_deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.agg(F.max(F.length(df_new[\"short_description\"]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"name_normalized\", \"country_of_incorporation\", \"region\").agg(F.count(\"name_normalized\").alias(\"name_count\")).orderBy(F.desc(\"name_count\")).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.withColumn(\"description_length\",F.length(df_new[\"short_description\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.select(F.max(\n",
    "    F.struct(\"description_length\", *(x for x in df_new.columns if x != \"description_length\"))\n",
    ")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len = {}\n",
    "for column in string_columns:\n",
    "    if column == \"name_normalized\":\n",
    "        continue\n",
    "    avg_len[column] = df_new.agg(F.mean(F.length(df_new[column]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len_ori = {}\n",
    "for column in string_columns:\n",
    "    avg_len_ori[column] = df.agg(F.mean(F.length(df[column]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
