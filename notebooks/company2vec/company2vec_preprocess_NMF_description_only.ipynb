{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "* make this look at description only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    " \"executorCores\": 4,\n",
    " \"executorMemory\": \"47696M\",\n",
    " \"conf\": {\"spark.default.parallelism\": 1000,\n",
    "          \"spark.sql.shuffle.partitions\": 1000,\n",
    "          \"spark.task.cpus\": 1\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id_path = \"s3://onai-ml-dev-eu-west-1/company2vec/common\"\n",
    "data_path = \"s3://onai-ml-dev-eu-west-1/company2vec/data_nmf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import lemmatize, tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from langdetect import detect\n",
    "import string\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import smart_open\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "from pyspark.ml.feature import (HashingTF,\n",
    "                                IDF,\n",
    "                                Tokenizer,\n",
    "                                StopWordsRemover,\n",
    "                                CountVectorizer,\n",
    "                                StringIndexer,\n",
    "                                OneHotEncoderEstimator,\n",
    "                                VectorAssembler,\n",
    "                                VectorSizeHint,\n",
    "                                StandardScaler,\n",
    "                                PCA\n",
    "                               )\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.mllib.linalg import DenseMatrix\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_raw = spark.read.load(\"s3://ai-data-lake-dev-eu-west-1/business/company_data_denormalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "is_english_udf = F.udf(is_english, T.BooleanType())\n",
    "\n",
    "p = PorterStemmer()\n",
    "def process_text(text):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = remove_stopwords(text)\n",
    "    text = p.stem_sentence(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in tokenize(text, lower=True)]\n",
    "    return list(\n",
    "        filter(lambda word: word not in string.punctuation and word.isalpha() and len(word) > 1, words)\n",
    "    )\n",
    "process_text_udf = F.udf(process_text, T.ArrayType(T.StringType()))\n",
    "\n",
    "def sparse_bow(*args):\n",
    "    ret = []\n",
    "    for el in args:\n",
    "        if not el:\n",
    "            continue\n",
    "        ret.extend(el)\n",
    "    return dict(Counter(ret))\n",
    "sparse_bow_udf = F.udf(sparse_bow, T.MapType(T.StringType(), T.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoderEmpty(Transformer):\n",
    "    def __init__(self, inputCol, outputCol, categories):\n",
    "        super(OneHotEncoderEmpty, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.categories = categories\n",
    "\n",
    "    def _transform(self, ds):\n",
    "        categories = self.categories\n",
    "\n",
    "        def fill_onehot(text):\n",
    "            ret = [0.0]*len(categories)\n",
    "            if not text:\n",
    "                return ret\n",
    "            for i,el in enumerate(categories):\n",
    "                if text == el:\n",
    "                    ret[i]=1.0\n",
    "            return ret\n",
    "\n",
    "        fill_onehot_udf = F.udf(fill_onehot, T.ArrayType(T.DoubleType()))\n",
    "        onehot_to_vector = F.udf(lambda arr: Vectors.dense(arr), VectorUDT())\n",
    "\n",
    "        ds = ds.withColumn(self.outputCol+\"_tmp\", fill_onehot_udf(self.inputCol))\n",
    "        return ds.withColumn(self.outputCol, onehot_to_vector(self.outputCol+\"_tmp\")).drop(self.outputCol+\"_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_ids = [row.company_id for row in\n",
    "                 companies_raw\n",
    "                 .groupBy(\"company_id\")\n",
    "                 .agg(F.count(\"*\").alias(\"count\"))\n",
    "                 .filter(F.col(\"count\") > 1)\n",
    "                 .collect()\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = (companies_raw\n",
    "             .filter(~F.col(\"company_id\").isin(duplicate_ids))\n",
    "             .filter((F.col(\"company_long_description\").isNotNull() &\n",
    "                     (F.length(\"company_long_description\") > 0)) |\n",
    "                     \n",
    "                     (F.col(\"company_description\").isNotNull() &\n",
    "                     (F.length(\"company_description\") > 0)) |\n",
    "                     \n",
    "                     (F.col(\"website_long_description\").isNotNull() &\n",
    "                     (F.length(\"website_long_description\") > 0)) |\n",
    "                     \n",
    "                     (F.col(\"website_description\").isNotNull() &\n",
    "                     (F.length(\"website_description\") > 0))\n",
    "                    )\n",
    "             .withColumn(\"merged_description\",\n",
    "                         sparse_bow_udf(process_text_udf(\"company_long_description\"),\n",
    "                                        process_text_udf(\"company_description\"),\n",
    "                                        process_text_udf(\"website_long_description\"),\n",
    "                                        process_text_udf(\"website_description\"))\n",
    "                        )\n",
    "             .filter(F.size(\"merged_description\") > 0)\n",
    "             .drop(\"company_description\")\n",
    "             .drop(\"company_long_description\")\n",
    "             .drop(\"website_description\")\n",
    "             .drop(\"website_long_description\")\n",
    "             .fillna({\"latest_ebitda\": 0.0,\n",
    "                      \"latest_revenue\": 0.0,\n",
    "                      \"latest_revenue_growth\": 0.0,\n",
    "                      \"latest_ebitda_margin\": 0.0,\n",
    "                      \"number_of_employees\": 0\n",
    "                     })\n",
    "             .repartition(1000)\n",
    "             .cache()\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.select(\"company_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = list(\n",
    "    sorted([row[0] for row in companies.select(\"country\").distinct().collect() if len(row[0]) > 0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_ids = list(\n",
    "    sorted([row[0] for row in companies.select(\"sic_code\").distinct().collect() if len(row[0]) > 0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_types = list(\n",
    "    sorted([row[0] for row in companies.select(\"company_type\").distinct().collect() if len(row[0]) > 0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = list(\n",
    "    sorted([row[0] for row in companies.select(\"region\").distinct().collect() if len(row[0]) > 0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = companies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = num_docs*0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = {row.key: row.n_docs for row in\n",
    "            companies.select(F.explode(\"merged_description\"))\n",
    "                     .groupBy(\"key\")\n",
    "                     .agg(F.count(\"*\").alias(\"n_docs\"))\n",
    "                     .filter(F.col(\"n_docs\") > min_df)\n",
    "                     .collect()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_idf = {k: math.log((num_docs+1)/(v+1)) for k,v in words_df.items()}\n",
    "idx = 0\n",
    "id2word = {}\n",
    "word2id = {}\n",
    "for word in sorted(words_idf):\n",
    "    id2word[idx] = word\n",
    "    word2id[word] = idx\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with smart_open.open(f\"{word2id_path}/bow/words_idf.csv\", \"w\") as f:\n",
    "    for word,idf in sorted(words_idf.items()):\n",
    "        f.write(f\"{word},{idf}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with smart_open.open(f\"{word2id_path}/bow/word2id.csv\", \"w\") as f:\n",
    "    for word,idd in sorted(word2id.items()):\n",
    "        f.write(f\"{word},{idd}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_idf = {}\n",
    "with smart_open.open(f\"{word2id_path}/bow/words_idf.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        word,idf = line.strip().split(\",\")\n",
    "        words_idf[word] = float(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "with smart_open.open(f\"{word2id_path}/bow/word2id.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        word,idd = line.strip().split(\",\")\n",
    "        word2id[word] = int(idd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_tfidf(bow):\n",
    "    dct = {word2id[k]: math.log(v+1)*words_idf[k] for k,v in bow.items() if k in word2id}\n",
    "    return Vectors.sparse(num_words, dct)\n",
    "words_tfidf_udf = F.udf(words_tfidf, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_tfidf = (companies\n",
    "                   .withColumn(\"bow_tfidf\", words_tfidf_udf(\"merged_description\"))\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_tfidf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_onehot_encoder = OneHotEncoderEmpty(inputCol=\"country\", \n",
    "                                            outputCol=\"country_onehot\",\n",
    "                                            categories=countries\n",
    "                                            )\n",
    "industry_onehot_encoder = OneHotEncoderEmpty(inputCol=\"sic_code\", \n",
    "                                            outputCol=\"industry_onehot\",\n",
    "                                            categories=industry_ids\n",
    "                                            )\n",
    "type_onehot_encoder = OneHotEncoderEmpty(inputCol=\"company_type\", \n",
    "                                         outputCol=\"type_onehot\",\n",
    "                                         categories=company_types\n",
    "                                         )\n",
    "region_onehot_encoder = OneHotEncoderEmpty(inputCol=\"region\", \n",
    "                                           outputCol=\"region_onehot\",\n",
    "                                           categories=regions\n",
    "                                           )\n",
    "\n",
    "vectorizer_numeric = VectorAssembler(inputCols=[\n",
    "                                        \"latest_revenue\",\n",
    "                                        \"number_of_employees\",\n",
    "                                        \"latest_ebitda\",\n",
    "                                        \"latest_ebitda_margin\",\n",
    "                                        \"latest_revenue_growth\"\n",
    "                                       ],\n",
    "                             outputCol=\"numeric_features\",\n",
    "                             handleInvalid = \"skip\"\n",
    "                            )\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "vectorizer = VectorAssembler(inputCols=[\"bow_tfidf\",\n",
    "                                        \"scaledFeatures\",\n",
    "                                        \"industry_onehot\",\n",
    "                                        \"region_onehot\",\n",
    "                                        \"type_onehot\",\n",
    "                                        \"country_onehot\"\n",
    "                                       ],\n",
    "                             outputCol=\"features\",\n",
    "                             handleInvalid = \"skip\"\n",
    "                            )\n",
    "\n",
    "pipeline = Pipeline(stages=[country_onehot_encoder,\n",
    "                            region_onehot_encoder,\n",
    "                            industry_onehot_encoder,\n",
    "                            type_onehot_encoder,\n",
    "                            vectorizer_numeric,\n",
    "                            scaler,\n",
    "                            vectorizer\n",
    "                           ])\n",
    "pipeline_fit = pipeline.fit(companies_tfidf)\n",
    "processed_companies = pipeline_fit.transform(companies_tfidf).repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_checker_udf(categories):    \n",
    "    def check_onehot(column, vector):\n",
    "        if not column:\n",
    "            return not np.any(vector)\n",
    "        i=-1\n",
    "        for ii,cat in enumerate(categories):\n",
    "            if cat == column:\n",
    "                i=ii\n",
    "                break\n",
    "        return bool(vector[i] == 1.0)\n",
    "    return F.udf(check_onehot, T.BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(processed_companies.select(\"country\",\n",
    "                            \"country_onehot\",\n",
    "                            make_checker_udf(countries)(\"country\", \"country_onehot\").alias(\"country_valid\"),\n",
    "                            \"sic_code\",\n",
    "                            \"industry_onehot\",\n",
    "                            make_checker_udf(industry_ids)(\"sic_code\", \"industry_onehot\").alias(\"industry_valid\")\n",
    "                           )\n",
    ").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some columns can be negative, so for them we subtract the minimum from them, so we can apply NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_splat(vec):\n",
    "    ret = {}\n",
    "    for ind,val in zip(vec.indices, vec.values):\n",
    "        ret[int(ind)] = float(val)\n",
    "    return ret\n",
    "\n",
    "col_splat_udf = F.udf(col_splat, T.MapType(T.IntegerType(), T.DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = {row.key: min(0.0, row.min_val_at_index) \n",
    "           for row in\n",
    "            processed_companies.select(col_splat_udf(\"features\").alias(\"sparse\"))\n",
    "             .select(F.explode(\"sparse\"))\n",
    "             .groupBy(\"key\")\n",
    "             .agg(F.min(\"value\").alias(\"min_val_at_index\"))\n",
    "             .collect()\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_size = F.udf(lambda vec: vec.size, T.IntegerType())\n",
    "get_indices = F.udf(lambda vec: [int(el) for el in vec.indices], T.ArrayType(T.IntegerType()))\n",
    "get_values  = F.udf(lambda vec: [float(el) - min_val[vec.indices[i]] for i,el in enumerate(vec.values)], T.ArrayType(T.DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(processed_companies.select(\"company_id\",\n",
    "                            get_size(\"features\").alias(\"size\"),\n",
    "                            get_indices(\"features\").alias(\"feature_indices\"),\n",
    "                            get_values(\"features\").alias(\"feature_values\"),\n",
    "                            \"merged_description\"\n",
    "                           )\n",
    " .repartition(32)\n",
    " .write\n",
    " .parquet(f\"{data_path}/raw_company_features\", mode=\"overwrite\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_df = spark.read.load(f\"{data_path}/raw_company_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_df.filter(\"company_id == '704634'\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_df.select(\"company_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
